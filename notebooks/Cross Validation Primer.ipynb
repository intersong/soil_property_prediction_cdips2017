{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# There's No Such Thing as a Free Lunch\n",
    "\n",
    "In the field of machine learning, the \"no free lunch\" theorem \"state[s] that any two optimization \n",
    "algorithms are equivalent when their performance is averaged across all possible problems\" (see [1] \n",
    "and [2] for some references).That is, you can't tell which machine learning algorithm will best solve the problem in front of you. Of course, with more experience you'll be able to \"intuit\" which ones are better \n",
    "suited than others, but a priori you can't say which is the best one.\n",
    "\n",
    "So we then need a way to measure if machine learning algorithm (A) is better than machine learning \n",
    "algorithm (B). Or for that matter, if (A) with parameters {l,m,n} or (A) with parameters {p,q,r} will do better. This \n",
    "is where **cross-validation** comes in.\n",
    "\n",
    "[1] Wolpert, David (1996), \"The Lack of A Priori Distinctions between Learning Algorithms\", Neural Computation, pp. 1341-1390\n",
    "\n",
    "[2] Wolpert, D.H., and Macready, W.G. (2005) \"Coevolutionary free lunches\", IEEE Transactions on Evolutionary Computation, 9(6): 721-735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Cross-validation is a way to measure how well a machine learning algorithm will generalize to more data (i.e the \"real\" dataset). For prediction problems, the steps typically go like this:\n",
    "1. Prepare a set of data where the **target** (the value you want to predict) is already known. \n",
    "2. Split this set into a **training set** and a **validation (or test) set**\n",
    "3. Fit your model to the **training set**, and use the fit to predict values for the **validation set**\n",
    "4. See how well the model predicted the **validation set**\n",
    "\n",
    "A typical measure is the **coefficient of determination, R^2**, which is defined as below:\n",
    "\n",
    "![R^2 definition](img/R2def.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's See Why We Need Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import scripts.load_data as load\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 10\n",
    "x_plot = np.linspace(1,100,100)\n",
    "x_train = x_plot[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = x_plot[N_split:]\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,5])\n",
    "lw =2\n",
    "for count, degree in enumerate([2, 4, 5, N_split-1]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=\"degree %d with score %.02f\" %(degree, score))\n",
    "\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking a Good Training Set\n",
    "\n",
    "The example above is meant to illustrate two things:\n",
    "1. Training on too little data yields poor results. (Typically people use 80%train, 20% validate)\n",
    "2. Even a good split won't save you if your training set is not representative of the data. Picking a bad training set will bias your model.\n",
    "\n",
    "The example below shows the importance of shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this the same code as above\n",
    "#except see how x_train is chosen differently\n",
    "\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 70\n",
    "x_plot = np.linspace(1,100,num=100)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x_train = np.sort(x[:N_split])\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = np.sort(x[N_split:])\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,7])\n",
    "lw =2\n",
    "for count, degree in enumerate([2, 4, 5, N_split-1]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=\"degree %d with score %0.02f /n naive score %0.02f\" %(degree, score, nscore))\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using k-Fold Cross Validation to avoid over fitting\n",
    "\n",
    "Notice that while the 9-degree polynomial goes through every data point in the training set perfectly, it does not generalize to new data very well. If you took the performance of the fit to the training data as the measure of the model's success, you would c\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(model, x.reshape(-1,1), data_model(x).reshape(-1,1), scoring='r2') #cv=5 for five-fold default is 3-fold\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scripts.load_data import*\n",
    "data, targets = load_training_spectra()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = targets.pH #for example just try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks = np.array_split(data,5)#split data into 5 equal size chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunks[4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_chunks = np.array_split(targets,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    data_t = pd.concat(data_chunks[j] for j in range(5) if j != i) #train\n",
    "    target_t = pd.concat(target_chunks[j] for j in range(5) if j != i)\n",
    "    data_v = data_chunks[i] #validate\n",
    "    target_v = target_chunks[i]\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(data_t, target_t)\n",
    "    print(rf.score(data_v, target_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf2 = RandomForestRegressor()\n",
    "score = cross_val_score(rf2, data, targets, scoring='r2') #cv=5 for five-fold default is 3-fold\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score), np.std(score)/np.sqrt(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cdips]",
   "language": "python",
   "name": "conda-env-cdips-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
