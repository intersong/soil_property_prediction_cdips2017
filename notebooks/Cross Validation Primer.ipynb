{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# There's No Such Thing as a Free Lunch\n",
    "\n",
    "In the field of machine learning, the \"no free lunch\" theorem \"state[s] that any two optimization \n",
    "algorithms are equivalent when their performance is averaged across all possible problems\" (see [1] \n",
    "and [2] for some references).That is, we can't tell which machine learning algorithm will best solve the problem in front of us. Of course, with more experience we'll be able to \"intuit\" which ones are better \n",
    "suited than others, but a priori we can't say which is the best one.\n",
    "\n",
    "So we then need a way to measure if machine learning algorithm (A) is better than machine learning \n",
    "algorithm (B). Or for that matter, if (A) with parameters {l,m,n} or (A) with parameters {p,q,r} will do better. This \n",
    "is where **cross-validation** comes in.\n",
    "\n",
    "[1] Wolpert, David (1996), \"The Lack of A Priori Distinctions between Learning Algorithms\", Neural Computation, pp. 1341-1390\n",
    "\n",
    "[2] Wolpert, D.H., and Macready, W.G. (2005) \"Coevolutionary free lunches\", IEEE Transactions on Evolutionary Computation, 9(6): 721-735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Cross-validation is a way to measure how well a machine learning algorithm will generalize to more data (i.e the \"real\" dataset). For prediction problems, the steps typically go like this:\n",
    "1. Prepare a set of data where the **target** (the value we want to predict) is already known. \n",
    "2. Split this set into a **training set** and a **validation (or test) set**\n",
    "3. Fit our model to the **training set**, and use the fit to predict values for the **validation set**\n",
    "4. See how well the model predicted the **validation set**\n",
    "\n",
    "A typical measure is the **coefficient of determination, R^2**, which is defined as below:\n",
    "\n",
    "![R^2 definition](img/R2def.png)\n",
    "\n",
    "\n",
    "The examples below go trough picking a good training set, demonstrating the need for cross validation, and showing\n",
    "a techniquie called k-fold cross validation that is especially important when there is not a lot of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking a Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import scripts.load_data as load\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#set the seed so we get the same data each time we run the cell\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 10\n",
    "x_plot = np.linspace(1,100,100)\n",
    "x_train = x_plot[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = x_plot[N_split:]\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,5])\n",
    "lw =2\n",
    "for count, degree in enumerate([2, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=\"degree %d\" %(degree))\n",
    "\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't Evaluate The \"Goodness\" of a Model on Training Data Only\n",
    "\n",
    "The example above is meant to illustrate two things:\n",
    "1. Training on too little data yields poor results. (Typically people use 80% train, 20% validate)\n",
    "2. Even if the split is sufficient, the training set must be devised to be representative of the data. Picking a bad training set will bias the model.\n",
    "\n",
    "The example below shows the importance of evaluating our model on a reserved validation set, i.e the importance of\n",
    "cross validation. Even if we have a \"good\" training set, calculating R^2 for the training set is not sufficient (I\n",
    "refer to this as a 'naive score' below). We must fit the training data, and calculate R^2 on a test set (also sometimes called a \"hold-out\" set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this the same code as above\n",
    "#except see how x_train is chosen differently\n",
    "\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 80\n",
    "x_plot = np.linspace(1,100,num=100)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = x[N_split:]\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,6])\n",
    "lw =2\n",
    "\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()), # Overly Simple\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()), # Just Right\n",
    "    DecisionTreeRegressor(max_depth=80)  # Overly Complex\n",
    "]\n",
    "\n",
    "labels = ['overly simple', 'just right', 'overly complex']\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with CV score %0.02f \\n   naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using K-Fold Cross Validation\n",
    "\n",
    "The example above showed why we need a good training set and a validation set to cross validate the model. \n",
    "If we only looked at the 'naive score', we would think the best fit is the overly-complex model because \n",
    "it has a \"perfect score\" of 1.0.\n",
    "\n",
    "When we only have a few data points, we can have a good, representative training set, a good train/validate split, a good CV score and still pick the wrong model!\n",
    "\n",
    "Leaving a specific point in the validation set biases the cross validation score because that single, \n",
    "specific data point is given too much importance / statistical weight. To avoid this, a method called\n",
    "**k-fold cross validation** is employed. K-fold cross validation picks _k_ training and validation sets \n",
    "from your data, ensuring that each point takes a turn in a training set and validation set. The average\n",
    "of the k-fold cross validation scores are taken to indicate which model is the best.\n",
    "\n",
    "In the plots below, take note of the effect on CV score of having a specific data point in the validation set.\n",
    "\n",
    "It should be noted that there are multiple methods that achieve the same results as k-fold cross \n",
    "validation. K-fold is one of the simplest to employ in scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this the same code as above\n",
    "#but we do it twice\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "###**TRAINING SET 1 *** ###\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 8\n",
    "x_plot = np.linspace(1,100,num=10)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = np.sort(x[N_split:])\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "print(x_val, y_val)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([-1,6])\n",
    "lw =2\n",
    "\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(7), Ridge())  \n",
    "]\n",
    "\n",
    "labels = ['2 degree', '4 degree', '7 degree']\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with CV score %0.02f \\n   naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.title('Training Set #1')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "###**TRAINING SET 2 *** ###\n",
    "###try another training & validation split\n",
    "rng = np.random.RandomState(4)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = np.sort(x[N_split:])\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "print(x_val, y_val)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([-1,6])\n",
    "lw =2\n",
    "\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with CV score %0.02f \\n   naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.title('Training Set #2')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "###Using k-fold cross validation\n",
    "def mean_cv_score(model):\n",
    "    return cross_val_score(model, x[:,np.newaxis] , data_model(x), scoring='r2').mean() #default is 3-fold\n",
    "\n",
    "cvs_means = [mean_cv_score(model) for model in models]\n",
    "poly_degree = [2,4,7]\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(poly_degree, cvs_means, label='Mean k-fold cross validation scores', marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean k-fold CV Score')\n",
    "plt.title('3-fold CV Scores')\n",
    "plt.legend(loc='lower left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set 1** suggests that the best choice is 4-degree.\n",
    "\n",
    "**Training Set 2** suggests that the best choice is 7-degree.\n",
    "\n",
    "k-fold CV confirms that of the models tested, 4-degree is the best choice. Try calculating the k-fold CV scores\n",
    "for 3-, 5-, and 6- polynomicals, and adding them to the last plot.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cdips]",
   "language": "python",
   "name": "conda-env-cdips-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
