{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# There's No Such Thing as a Free Lunch\n",
    "\n",
    "In the field of machine learning, the \"no free lunch\" theorem \"state[s] that any two optimization \n",
    "algorithms are equivalent when their performance is averaged across all possible problems\" (see [1] \n",
    "and [2] for some references). That is, we can't tell which machine learning algorithm will best solve the problem in front of us. Of course, with more experience we'll be able to \"intuit\" which ones are better \n",
    "suited than others, but a priori we can't say which is the best one.\n",
    "\n",
    "So we then need a way to measure if machine learning algorithm (A) is better than machine learning \n",
    "algorithm (B). Or for that matter, if (A) with parameters {l,m,n} or (A) with parameters {p,q,r} will do better. This \n",
    "is where **cross-validation** comes in.\n",
    "\n",
    "[1] Wolpert, David (1996), \"The Lack of A Priori Distinctions between Learning Algorithms\", Neural Computation, pp. 1341-1390\n",
    "\n",
    "[2] Wolpert, D.H., and Macready, W.G. (2005) \"Coevolutionary free lunches\", IEEE Transactions on Evolutionary Computation, 9(6): 721-735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Cross-validation is a way to measure how well a machine learning algorithm will generalize to more data. It's a way to simulate going out and collecting more data, without the headache of actually collecting more data. And it is meant to prevent the embarassing (and potentially expensive!) moment when you realize the model you trained is not actually a good fit for your big dataset.\n",
    "\n",
    "For prediction problems, the steps typically go like this:\n",
    "1. Prepare a set of data where the **target** (the value we want to predict) is already known. \n",
    "2. Split this set into a **training set** and a **validation (or test) set**\n",
    "3. Fit a model to the **training set**, and use the fit to predict values for the **validation set**\n",
    "4. See how well the model predicts the **validation set**\n",
    "\n",
    "A typical measure of model goodness is the **coefficient of determination, $R^2$**, which is defined as below:\n",
    "\n",
    "\n",
    "\n",
    "## $R^{2}(y, \\hat{y}) = 1 - \\frac{\\Sigma_{i=1}^{n_{samples}} (y_{i} - \\hat{y}_{i})^{2}}{\\Sigma_{i=1}^{n_{samples}}(y_{i}-\\bar{y})^{2}}$\n",
    "\n",
    "- $\\hat{y}_{i}$ is the predicted value of $i$-th sample\n",
    "\n",
    "- $y_{i}$ is the true value of $i$-th sample\n",
    "\n",
    "- ### $\\bar{y} = \\frac{1}{n_{samples}} \\Sigma_{i=1}^{n_{samples}} y_{i}$\n",
    "\n",
    "The examples below go through picking a good training set, demonstrating the need for cross validation, and showing\n",
    "a techniquie called k-fold cross validation that is especially important when there is not a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking a Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import scripts.load_data as load\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#set the seed so we get the same data each time we run the cell\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 10\n",
    "x_plot = np.linspace(1,100,100)\n",
    "x_train = x_plot[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = x_plot[N_split:]\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.title('Picking a Representative Training Set',  fontsize=20)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,5])\n",
    "lw =2\n",
    "for count, degree in enumerate([2, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=\"degree %d\" %(degree))\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't Evaluate The \"Goodness\" of a Model on Training Data Only\n",
    "\n",
    "The example above is meant to illustrate two things:\n",
    "1. Training on too little data yields poor results. (Typically people use 80% train, 20% validate)\n",
    "2. Even if the split is sufficient, the training set must be devised to be representative of the data. Picking a bad training set will bias the model.\n",
    "\n",
    "The example below shows the importance of evaluating our model on a reserved validation set, i.e the importance of\n",
    "cross validation. Even if we have a \"good\" training set, calculating $R^{2}$ for the training set is not sufficient (reffered to as 'naive score' below). We must fit the training data, and calculate $R^{2}$ on a test set (also sometimes called a \"hold-out\" set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this the same code as above\n",
    "#except see how x_train is chosen differently\n",
    "\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 80\n",
    "x_plot = np.linspace(1,100,num=100)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = x[N_split:]\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,10))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.title('Why We Need Cross Validation', fontsize=20)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,6])\n",
    "lw =2\n",
    "\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()), # Overly Simple\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()), # Just Right\n",
    "    DecisionTreeRegressor(max_depth=80)  # Overly Complex\n",
    "]\n",
    "\n",
    "labels = ['overly simple', 'just right', 'overly complex']\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with CV score %0.02f \\n   naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=16);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above shows why we need a good training set and a validation set to cross validate the model. \n",
    "If we only looked at the 'naive score', we would think the best fit is the overly-complex model because \n",
    "it has a \"perfect score\" of 1.0. But the relatively low cross validation score (0.90) of the overly-complex model\n",
    "indicates it doesn't generalize to new data, so it's not the best choice.\n",
    "\n",
    "The next section gives more examples and introdues the concept of **k-fold cross validation**.\n",
    "\n",
    "\n",
    "# Why K-Fold Cross Validation?\n",
    "\n",
    "When we only have a few data points, we have to be even more careful. We can have a good, representative training set, a good train/validate split, a good CV score and _still_ pick the wrong model!\n",
    "\n",
    "Leaving a specific point in the validation set biases the cross validation score because that single, \n",
    "specific data point is given too much importance / statistical weight. To avoid this, a method called\n",
    "**k-fold cross validation** is employed. K-fold cross validation picks _k_ training and validation sets \n",
    "from the data, ensuring that each point takes a turn in a training set and validation set. The average\n",
    "of the k-fold cross validation scores is taken to indicate which model is the best.\n",
    "\n",
    "In the plots below, take note of the effect on CV score of having a specific data point in the validation set.\n",
    "\n",
    "It should be noted that there are multiple methods that achieve the same results as k-fold cross \n",
    "validation. K-fold is one of the simplest to employ in scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this the same code as above\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "###**TRAINING SET 1 *** ###\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 8\n",
    "x_plot = np.linspace(1,100,num=10)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = np.sort(x[N_split:])\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,10))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([-1,6])\n",
    "lw =2\n",
    "\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(7), Ridge())  \n",
    "]\n",
    "\n",
    "labels = ['2 degree', '4 degree', '7 degree']\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with CV score %0.02f \\n   naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.title('Training Set #1', fontsize=20)\n",
    "plt.legend(loc='lower right', fontsize=16);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set 1** (above) suggests that the best choice is 4-degree becaus it has the best CV score. The 4-degree polynomial fits the training data well, but by the choice of validation set it _looks_ like it does not generalize very well when presented with a new data point down near (0,0). Let's see the effect of trying a different training set.\n",
    "\n",
    "## Training Set 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###**TRAINING SET 2 *** ###\n",
    "###try another training & validation split\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "x_plot = np.linspace(1,100,num=10)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(5)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = np.sort(x[N_split:])\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,10))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([-1,6])\n",
    "lw =2\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(7), Ridge())  \n",
    "]\n",
    "\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with CV score %0.02f \\n   naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.title('Training Set #2', fontsize=20)\n",
    "plt.legend(loc='lower right', fontsize=16);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Set 2** suggests that the best choice is 7-degree because it has the best CV score. However, its naive score is 1.0, which is a pretty good indication that it is probably over-fitting. Note that the 4-degree polynomial fit returned different model parameters this time, and it actually does fit the point near (0,0). \n",
    "\n",
    "These examples are meant to illustrate how a specific point being in the validation set or not can result in different CV scores for the same model. I.e the particular validation set is a source of bias. So which model is correct!?\n",
    "\n",
    "The way we deal with this is by doing **k-fold cross validatoin** -- doing several cross validations and averaging the scores. This gives each point a 'turn' in the validation data, and averaging over the scores ensures that any single data point doesn't determine which model we ultimately choose. \n",
    "\n",
    "\n",
    "## K-Fold Cross Validation Example\n",
    "\n",
    "The cell below shows how to use k-fold cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Using k-fold cross validation\n",
    "\n",
    "np.random.seed(0)\n",
    "x_plot = np.linspace(1,100,num=10)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "rng.shuffle(x)\n",
    "\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(7), Ridge())  \n",
    "]\n",
    "\n",
    "def mean_cv_score(model):\n",
    "    return cross_val_score(model, x[:,np.newaxis] , data_model(x), scoring='r2', cv=5).mean() #default is 3-fold\n",
    "\n",
    "cvs_means = [mean_cv_score(model) for model in models]\n",
    "print(cvs_means)\n",
    "poly_degree = [2,4,7]\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(poly_degree, cvs_means, label='Mean k-fold cross validation scores', marker='o')\n",
    "plt.xlabel('Polynomial Degree', fontsize=20)\n",
    "plt.ylabel('Mean k-fold CV Score', fontsize=20)\n",
    "plt.title('5-fold CV Scores', fontsize=20)\n",
    "plt.legend(loc='lower left', fontsize=16);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-fold cross validation** in the plot above confirms that of the models tested, 4-degree is the best choice. \n",
    "\n",
    "Try calculating the k-fold CV scores for 3-, 5-, and 6- degree polynomials, and adding them to the last plot.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cdips]",
   "language": "python",
   "name": "conda-env-cdips-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
