{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# There's No Such Thing as a Free Lunch\n",
    "\n",
    "In the field of machine learning, the \"no free lunch\" theorem \"state[s] that any two optimization \n",
    "algorithms are equivalent when their performance is averaged across all possible problems\" (see [1] \n",
    "and [2] for some references).That is, you can't tell which machine learning algorithm will best solve the problem in front of you. Of course, with more experience you'll be able to \"intuit\" which ones are better \n",
    "suited than others, but a priori you can't say which is the best one.\n",
    "\n",
    "So we then need a way to measure if machine learning algorithm (A) is better than machine learning \n",
    "algorithm (B). Or for that matter, if (A) with parameters {l,m,n} or (A) with parameters {p,q,r} will do better. This \n",
    "is where **cross-validation** comes in.\n",
    "\n",
    "[1] Wolpert, David (1996), \"The Lack of A Priori Distinctions between Learning Algorithms\", Neural Computation, pp. 1341-1390\n",
    "\n",
    "[2] Wolpert, D.H., and Macready, W.G. (2005) \"Coevolutionary free lunches\", IEEE Transactions on Evolutionary Computation, 9(6): 721-735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Cross-validation is a way to measure how well a machine learning algorithm will generalize to more data (i.e the \"real\" dataset). For prediction problems, the steps typically go like this:\n",
    "1. Prepare a set of data where the **target** (the value you want to predict) is already known. \n",
    "2. Split this set into a **training set** and a **validation (or test) set**\n",
    "3. Fit your model to the **training set**, and use the fit to predict values for the **validation set**\n",
    "4. See how well the model predicted the **validation set**\n",
    "\n",
    "A typical measure is the **coefficient of determination, R^2**, which is defined as below:\n",
    "\n",
    "![R^2 definition](img/R2def.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking a Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import scripts.load_data as load\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 10\n",
    "x_plot = np.linspace(1,100,100)\n",
    "x_train = x_plot[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = x_plot[N_split:]\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,5])\n",
    "lw =2\n",
    "for count, degree in enumerate([2, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=\"degree %d\" %(degree))\n",
    "\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't Evaluate The \"Goodness\" of your model on training data only\n",
    "\n",
    "The example above is meant to illustrate two things:\n",
    "1. Training on too little data yields poor results. (Typically people use 80%train, 20% validate)\n",
    "2. Even a good split won't save you if your training set is not representative of the data. Picking a bad training set will bias your model.\n",
    "\n",
    "The example below shows the importance of evaluating your model on a reserved validation set, i.e the importance of cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this the same code as above\n",
    "#except see how x_train is chosen differently\n",
    "\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 80\n",
    "x_plot = np.linspace(1,100,num=100)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = x[N_split:]\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([0,6])\n",
    "lw =2\n",
    "\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()), # Overly Simple\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()), # Just Right\n",
    "    DecisionTreeRegressor(max_depth=80)  # Overly Complex\n",
    "]\n",
    "\n",
    "labels = ['overly simple', 'just right', 'overly complex']\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with score %0.02f /n naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using k-Fold Cross Validation to avoid over fitting\n",
    "\n",
    "Leaving a specific point in the validation set biases the cross validation score because that single, specific data point is given too much importance / statistical weight. To avoid this, a method called **k-fold cross validation** is employed. K-fold cross validation picks _k_ training and validation sets from your data, ensuring that each point takes a turn in a training set and validation set. The average of the k-fold cross validation scores are taken to indicate which model is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this the same code as above\n",
    "#except see how x_train is chosen differently\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#set the seed you get the same data each time you run\n",
    "np.random.seed(0)\n",
    "\n",
    "#define a model to generate fake data\n",
    "def data_model(x, sigma=0.1):\n",
    "    base_model = np.log(x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    return base_model + noise\n",
    "\n",
    "#Generate some fake data and keep a subset of them\n",
    "#for training\n",
    "#***Try different values for N***\n",
    "N_split = 8\n",
    "x_plot = np.linspace(1,100,num=10)\n",
    "x = np.copy(x_plot)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = np.sort(x[N_split:])\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "print(x_val, y_val)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([-1,6])\n",
    "lw =2\n",
    "\n",
    "models = [\n",
    "    make_pipeline(PolynomialFeatures(2), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(4), Ridge()),\n",
    "    make_pipeline(PolynomialFeatures(7), Ridge())  \n",
    "]\n",
    "\n",
    "labels = ['2 degree', '4 degree', '7 degree']\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with score %0.02f /n naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "###try another training & validation split\n",
    "rng = np.random.RandomState(4)\n",
    "rng.shuffle(x)\n",
    "x_train = x[:N_split]\n",
    "y_train = data_model(x_train)\n",
    "\n",
    "#also make validation data\n",
    "x_val = np.sort(x[N_split:])\n",
    "y_val = data_model(x_val)\n",
    "# create matrix versions of these arrays\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_val = x_val[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'red']\n",
    "lw = 0\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(x_train, y_train, color='navy', s=30, marker='o', label=\"training points\", linewidth=lw)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "print(x_val, y_val)\n",
    "plt.scatter(x_val, y_val, marker='o', color='cornflowerblue', alpha=0.5,linewidth=lw, label=\"validation data\")\n",
    "\n",
    "plt.ylim([-1,6])\n",
    "lw =2\n",
    "\n",
    "#for count, degree in enumerate([2, 5, 12]):\n",
    "for count, model in enumerate(models):\n",
    "    #fit to training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_fit = model.predict(X_train)\n",
    "    nscore = skl.metrics.r2_score(y_train, y_train_fit)\n",
    "    #apply to validation data\n",
    "    y_predicted = model.predict(X_val)\n",
    "    #get R^2 score\n",
    "    score = skl.metrics.r2_score(y_val, y_predicted)\n",
    "    #apply again, just to show plot\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, alpha=0.5,\n",
    "             label=labels[count] + \" with score %0.02f /n naive score %0.02f\" %(score, nscore))\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "def mean_cv_score(model):\n",
    "    return cross_val_score(model, x[:,np.newaxis] , data_model(x), scoring='r2').mean()\n",
    "\n",
    "cvs_means = [mean_cv_score(model) for model in models]\n",
    "poly_degree = [2,4,7]\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(poly_degree, cvs_means, label='Mean k-fold cross validation scores')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean k-fold CV Score')\n",
    "plt.legend(loc='lower left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last plot shows that a 4 degree polynomial is a better choise than a 2 or 7 degree. But if you had only done one cross validation, it could have led you to believe a 7 degree polynomial is the best choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scripts.load_data import*\n",
    "data, targets = load_training_spectra()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = targets.pH #for example just try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks = np.array_split(data,5)#split data into 5 equal size chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks[4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_chunks = np.array_split(targets,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    data_t = pd.concat(data_chunks[j] for j in range(5) if j != i) #train\n",
    "    target_t = pd.concat(target_chunks[j] for j in range(5) if j != i)\n",
    "    data_v = data_chunks[i] #validate\n",
    "    target_v = target_chunks[i]\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(data_t, target_t)\n",
    "    print(rf.score(data_v, target_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf2 = RandomForestRegressor()\n",
    "score = cross_val_score(rf2, data, targets, scoring='r2') #cv=5 for five-fold default is 3-fold\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(score), np.std(score)/np.sqrt(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cdips]",
   "language": "python",
   "name": "conda-env-cdips-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
